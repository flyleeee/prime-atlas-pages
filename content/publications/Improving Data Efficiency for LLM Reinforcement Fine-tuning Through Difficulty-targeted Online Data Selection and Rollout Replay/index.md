---
publication_types:
  - "3"
authors:
  - Yifan Sun
  - Jingyan Shen
  - me
  - Tianyu Chen
  - Zhendong Wang
  - Mingyuan Zhou
  - Huan Zhang
author_notes:
  - Equal contribution
  - Equal contribution
  - Equal contribution
publication: "Advances in Neural Information Processing Systems (NeurIPS), 2025"
publication_short: "NeurIPS 2025"
abstract: Estimating the uncertainty of responses of Large Language Models~(LLMs) remains a critical challenge. While recent Bayesian methods have demonstrated effectiveness in quantifying uncertainty through low-rank weight updates, they typically require complex fine-tuning or post-training procedures. In this paper, we propose Training-Free Bayesianization~(TFB), a novel framework that transforms existing off-the-shelf trained LoRA adapters into Bayesian ones without additional training. TFB systematically searches for the maximally acceptable level of variance in the weight posterior, constrained within a family of low-rank isotropic Gaussian distributions. We theoretically demonstrate that under mild conditions, this search process is equivalent to variational inference for the weights. Through comprehensive experiments, we show that TFB achieves superior uncertainty estimation and generalization compared to existing methods while eliminating the need for complex training procedures. Code will be available at [this https URL](https://github.com/Wang-ML-Lab/bayesian-peft).
image:
  filename: avatar.jpg
  focal_point: Center
  preview_only: false
url_dataset: ""
url_project: ""
url_source: ""
url_video: ""
title: "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay"
doi: ""
featured: true
tags:
  - RL Fine-tuing
  - Data Selection
  - Large Language Models
projects: []
date: 2025-06-10T00:00:00Z
url_pdf: "https://arxiv.org/pdf/2506.05316"
url_slides: ""
publishDate: 2025-06-10T00:00:00Z
url_poster: 
url_code: 
links:
- name: arxiv
  url: https://arxiv.org/abs/2506.05316
---
